{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Draft Public TPU: Pmap Cookbook.ipynb","provenance":[],"collapsed_sections":[],"last_runtime":{"build_target":"//learning/deepmind/dm_python:dm_notebook3_tpu","kind":"private"}},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"LpPtl0n4rg6L","colab_type":"text"},"source":["# Colab JAX TPU Setup"]},{"cell_type":"code","metadata":{"id":"4DYY4Yyhq8vG","colab_type":"code","outputId":"a8c84290-b009-4974-c6c0-f8307f17382f","executionInfo":{"status":"ok","timestamp":1573855817696,"user_tz":480,"elapsed":37812,"user":{"displayName":"Anselm Levskaya","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB3HU6tluRkaDUKajXgduO8xUvMosX7wCUR9KJm=s64","userId":"09409386770882740563"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["# Grab newest JAX version.\n","!pip install --upgrade -q jax==0.1.51 jaxlib==0.1.33\n","\n","# Make sure the Colab Runtime is set to Accelerator: TPU.\n","import requests\n","import os\n","if 'TPU_DRIVER_MODE' not in globals():\n","  url = 'http://' + os.environ['COLAB_TPU_ADDR'].split(':')[0] + ':8475/requestversion/tpu_driver_nightly'\n","  resp = requests.post(url)\n","  TPU_DRIVER_MODE = 1\n","\n","# The following is required to use TPU Driver as JAX's backend.\n","from jax.config import config\n","config.FLAGS.jax_xla_backend = \"tpu_driver\"\n","config.FLAGS.jax_backend_target = \"grpc://\" + os.environ['COLAB_TPU_ADDR']\n","print(config.FLAGS.jax_backend_target)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["\u001b[K     |████████████████████████████████| 235kB 3.3MB/s \n","\u001b[K     |████████████████████████████████| 43.1MB 51kB/s \n","\u001b[?25h  Building wheel for jax (setup.py) ... \u001b[?25l\u001b[?25hdone\n","grpc://10.118.79.162:8470\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_4ware9HrjIk","colab_type":"text"},"source":["# Pmap CookBook"]},{"cell_type":"markdown","metadata":{"id":"sk-3cPGIBTq8","colab_type":"text"},"source":["This notebook is an introduction to writing single-program multiple-data (SPMD) programs in JAX, and executing them synchronously in parallel on multiple devices, such as multiple GPUs or multiple TPU cores. The SPMD model is useful for computations like training neural networks with synchronous gradient descent algorithms, and can be used for data-parallel as well as model-parallel computations.\n","\n","To run this notebook with any parallelism, you'll need multiple XLA devices available, e.g. with a multi-GPU machine or a Cloud TPU.\n","\n","The code in this notebook is simple. For an example of how to use these tools to do data-parallel neural network training, check out [the SPMD MNIST example](https://github.com/google/jax/blob/master/examples/spmd_mnist_classifier_fromscratch.py) or the much more capable [Trax library](https://github.com/tensorflow/trax/)."]},{"cell_type":"code","metadata":{"id":"Srs8W9F6Jo15","colab_type":"code","colab":{}},"source":["import jax.numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hBasY8p1JFId","colab_type":"text"},"source":["## Basics"]},{"cell_type":"markdown","metadata":{"id":"caPiPIWgM7-W","colab_type":"text"},"source":["### Pure maps, with no communication"]},{"cell_type":"markdown","metadata":{"id":"2e_06-OAJNyi","colab_type":"text"},"source":["A basic starting point is expressing parallel maps with [`pmap`](https://jax.readthedocs.io/en/latest/jax.html#jax.pmap):"]},{"cell_type":"code","metadata":{"id":"6gGT77cIImcE","colab_type":"code","colab":{}},"source":["from jax import pmap"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-yY3lOFpJIUS","colab_type":"code","outputId":"3cc7a65a-d80b-459b-925a-dada9b995e9d","executionInfo":{"status":"ok","timestamp":1573855830463,"user_tz":480,"elapsed":744,"user":{"displayName":"Anselm Levskaya","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB3HU6tluRkaDUKajXgduO8xUvMosX7wCUR9KJm=s64","userId":"09409386770882740563"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["result = pmap(lambda x: x ** 2)(np.arange(7))\n","print(result)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["[ 0  1  4  9 16 25 36]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"PgKNzxKPNEYA","colab_type":"text"},"source":["In terms of what values are computed, `pmap` is similar to `vmap` in that it transforms a function to map over an array axis:"]},{"cell_type":"code","metadata":{"id":"mmCMQ64QbAbz","colab_type":"code","outputId":"8ecd80f5-80b6-4807-a87a-7ac20e5ea019","executionInfo":{"status":"ok","timestamp":1573855832659,"user_tz":480,"elapsed":366,"user":{"displayName":"Anselm Levskaya","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB3HU6tluRkaDUKajXgduO8xUvMosX7wCUR9KJm=s64","userId":"09409386770882740563"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["from jax import vmap\n","\n","x = np.array([1., 2., 3.])\n","y = np.array([2., 4., 6.])\n","\n","print(vmap(np.add)(x, y))\n","print(pmap(np.add)(x, y))"],"execution_count":5,"outputs":[{"output_type":"stream","text":["[3. 6. 9.]\n","[3. 6. 9.]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"iZgTmx5pFd6z","colab_type":"text"},"source":["But `pmap` and `vmap` differ in in how those values are computed: where `vmap` vectorizes a function by adding a batch dimension to every primitive operation in the function (e.g. turning matrix-vector multiplies into matrix-matrix multiplies), `pmap` instead replicates the function and executes each replica on its own XLA device in parallel."]},{"cell_type":"code","metadata":{"id":"4N1--GgGFe9d","colab_type":"code","outputId":"a039213f-f4d8-4265-a950-d2e7a2a6bca4","executionInfo":{"status":"ok","timestamp":1573855834868,"user_tz":480,"elapsed":238,"user":{"displayName":"Anselm Levskaya","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB3HU6tluRkaDUKajXgduO8xUvMosX7wCUR9KJm=s64","userId":"09409386770882740563"}},"colab":{"base_uri":"https://localhost:8080/","height":476}},"source":["from jax import make_jaxpr\n","\n","def f(x, y):\n","  a = np.dot(x, y)\n","  b = np.tanh(a)\n","  return b\n","\n","xs = np.ones((8, 2, 3))\n","ys = np.ones((8, 3, 4))\n","\n","print(\"f jaxpr\")\n","print(make_jaxpr(f)(xs[0], ys[0]))\n","\n","print(\"vmap(f) jaxpr\")\n","print(make_jaxpr(vmap(f))(xs, ys))\n","\n","print(\"pmap(f) jaxpr\")\n","print(make_jaxpr(pmap(f))(xs, ys))"],"execution_count":6,"outputs":[{"output_type":"stream","text":["f jaxpr\n","{ lambda  ;  ; a b.\n","  let c = dot_general[ dimension_numbers=(((1,), (0,)), ((), ()))\n","                       precision=None ] a b\n","      d = tanh c\n","  in [d] }\n","\n","vmap(f) jaxpr\n","{ lambda  ;  ; a b.\n","  let c = dot_general[ dimension_numbers=(((2,), (1,)), ((0,), (0,)))\n","                       precision=None ] a b\n","      d = tanh c\n","  in [d] }\n","\n","pmap(f) jaxpr\n","{ lambda  ;  ; a b.\n","  let c = xla_pmap[ axis_name=<axis 0x7f2475d5e950>\n","                    axis_size=8\n","                    devices=None\n","                    backend=None ] a b\n","        { lambda  ;  ; a b.\n","          let c = dot_general[ dimension_numbers=(((1,), (0,)), ((), ()))\n","                               precision=None ] a b\n","              d = tanh c\n","          in [d] } [  ;  ]\n","  in [c] }\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BjDnQkzSa_vZ","colab_type":"text"},"source":["Notice that applying `vmap(f)` to these arguments leads to a `dot_general` to express the batch matrix multiplication in a single primitive, while applying `pmap(f)` instead leads to a primitive that calls replicas of the original `f` in parallel.\n","\n","There are also important constraints with using `pmap`:\n","1. `pmap` always maps over the leading axis of all of its arguments (while with `vmap` you can use `in_axes` to specify which axes get mapped),\n","2. with `pmap` the mapped axis size must be less than or equal to the number of XLA devices available (and for nested `pmap` functions, the product of the mapped axis sizes must be less than or equal to the number of XLA devices).\n","\n","You can use the output of a `pmap` function just like any other value:"]},{"cell_type":"code","metadata":{"id":"H4DXQWobOf7V","colab_type":"code","outputId":"d984939d-57f5-4238-ff1a-838cf5473fe6","executionInfo":{"status":"ok","timestamp":1573855839056,"user_tz":480,"elapsed":521,"user":{"displayName":"Anselm Levskaya","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB3HU6tluRkaDUKajXgduO8xUvMosX7wCUR9KJm=s64","userId":"09409386770882740563"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["y = pmap(lambda x: x ** 2)(np.arange(8))\n","z = y / 2\n","print(z)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["[ 0.   0.5  2.   4.5  8.  12.5 18.  24.5]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fM1Une9Rfqld","colab_type":"code","outputId":"31a9232a-02da-45c3-dfd2-855de015e43c","executionInfo":{"status":"ok","timestamp":1573855842002,"user_tz":480,"elapsed":637,"user":{"displayName":"Anselm Levskaya","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB3HU6tluRkaDUKajXgduO8xUvMosX7wCUR9KJm=s64","userId":"09409386770882740563"}},"colab":{"base_uri":"https://localhost:8080/","height":282}},"source":["import matplotlib.pyplot as plt\n","plt.plot(y)"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<matplotlib.lines.Line2D at 0x7f2465cdff28>]"]},"metadata":{"tags":[]},"execution_count":8},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"644UB23YfbW4","colab_type":"text"},"source":["But while the output here acts just like a NumPy ndarray, if you look closely it has a different type:"]},{"cell_type":"code","metadata":{"id":"59hnyVOtfavX","colab_type":"code","outputId":"039b416d-1e11-4392-d0ef-5153feafa478","executionInfo":{"status":"ok","timestamp":1573855845400,"user_tz":480,"elapsed":224,"user":{"displayName":"Anselm Levskaya","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB3HU6tluRkaDUKajXgduO8xUvMosX7wCUR9KJm=s64","userId":"09409386770882740563"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["y"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ShardedDeviceArray([ 0,  1,  4,  9, 16, 25, 36, 49], dtype=int32)"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"4brdSdeyf2MP","colab_type":"text"},"source":["A `ShardedDeviceArray` is effectively an `ndarray` subclass, but it's stored in pieces spread across the memory of multiple devices. Results from `pmap` functions are left sharded in device memory so that they can be operated on by subsequent `pmap` functions without moving data around, at least in some cases. But these results logically appear just like a single array.\n","\n","When you call a non-`pmap` function on a `ShardedDeviceArray`, like a standard `jax.numpy` function, communication happens behind the scenes to bring the values to one device (or back to the host in the case of the matplotlib function above):"]},{"cell_type":"code","metadata":{"id":"BSSllkblg9Rn","colab_type":"code","outputId":"2e073b35-85cd-4f7d-9115-1f4f0eff86e7","executionInfo":{"status":"ok","timestamp":1573855848314,"user_tz":480,"elapsed":222,"user":{"displayName":"Anselm Levskaya","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB3HU6tluRkaDUKajXgduO8xUvMosX7wCUR9KJm=s64","userId":"09409386770882740563"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["y / 2"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DeviceArray([ 0. ,  0.5,  2. ,  4.5,  8. , 12.5, 18. , 24.5], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"efyMSNGahq6f","colab_type":"code","outputId":"42b91171-1256-4434-da1d-e8a5575479cb","executionInfo":{"status":"ok","timestamp":1573855849098,"user_tz":480,"elapsed":223,"user":{"displayName":"Anselm Levskaya","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB3HU6tluRkaDUKajXgduO8xUvMosX7wCUR9KJm=s64","userId":"09409386770882740563"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["import numpy as onp\n","onp.sin(y)"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 0.        ,  0.84147098, -0.7568025 ,  0.41211849, -0.28790332,\n","       -0.13235175, -0.99177885, -0.95375265])"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"Ba4jwfkbOwXW","colab_type":"text"},"source":["Thinking about device memory is important to maximize performance by avoiding data transfers, but you can always fall back to treating arraylike values as (read-only) NumPy ndarrays and your code will still work.\n","\n","Here's another example of a pure map which makes better use of our multiple-accelerator resources. We can generate several large random matrices in parallel, then perform parallel batch matrix multiplication without any cross-device movement of the large matrix data:"]},{"cell_type":"code","metadata":{"id":"rWl68coLJSi7","colab_type":"code","outputId":"4903cadf-f48a-47f6-809e-2dcee92b284c","executionInfo":{"status":"ok","timestamp":1573855853513,"user_tz":480,"elapsed":1563,"user":{"displayName":"Anselm Levskaya","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB3HU6tluRkaDUKajXgduO8xUvMosX7wCUR9KJm=s64","userId":"09409386770882740563"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from jax import random\n","\n","# create 8 random keys\n","keys = random.split(random.PRNGKey(0), 8)\n","# create a 5000 x 6000 matrix on each device by mapping over keys\n","mats = pmap(lambda key: random.normal(key, (5000, 6000)))(keys)\n","# the stack of matrices is represented logically as a single array\n","mats.shape"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(8, 5000, 6000)"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"nH2gGNgfNOJD","colab_type":"code","outputId":"34cc9633-53ff-41cb-a2b1-8dfc96a20d85","executionInfo":{"status":"ok","timestamp":1573855855113,"user_tz":480,"elapsed":959,"user":{"displayName":"Anselm Levskaya","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB3HU6tluRkaDUKajXgduO8xUvMosX7wCUR9KJm=s64","userId":"09409386770882740563"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# run a local matmul on each device in parallel (no data transfer)\n","result = pmap(lambda x: np.dot(x, x.T))(mats)\n","result.shape"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(8, 5000, 5000)"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"MKTZ59iPNPi5","colab_type":"code","outputId":"96d4ae2f-11a9-423b-b364-bed321ca121e","executionInfo":{"status":"ok","timestamp":1573855856018,"user_tz":480,"elapsed":331,"user":{"displayName":"Anselm Levskaya","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB3HU6tluRkaDUKajXgduO8xUvMosX7wCUR9KJm=s64","userId":"09409386770882740563"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["# compute the mean on each device in parallel and print the results\n","print(pmap(np.mean)(result))"],"execution_count":14,"outputs":[{"output_type":"stream","text":["[1.1566595 1.1805978 1.2052746 1.2045677 1.1876795 1.2037715 1.2321935\n"," 1.2015157]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"26iH7sHFiz2l","colab_type":"text"},"source":["In this example, the large matrices never had to be moved between devices or back to the host; only one scalar per device was pulled back to the host."]},{"cell_type":"markdown","metadata":{"id":"MdRscR5MONuN","colab_type":"text"},"source":["### Collective communication operations"]},{"cell_type":"markdown","metadata":{"id":"bFtajUwp5WYx","colab_type":"text"},"source":["In addition to expressing pure maps, where no communication happens between the replicated functions, with `pmap` you can also use special collective communication operations.\n","\n","One canonical example of a collective, implemented on both GPU and TPU, is an all-reduce sum like `lax.psum`:"]},{"cell_type":"code","metadata":{"id":"d5s8rJVUORQ3","colab_type":"code","outputId":"338cded2-65df-4efa-d865-8d08cbe91c19","executionInfo":{"status":"ok","timestamp":1573855860088,"user_tz":480,"elapsed":246,"user":{"displayName":"Anselm Levskaya","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB3HU6tluRkaDUKajXgduO8xUvMosX7wCUR9KJm=s64","userId":"09409386770882740563"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from jax import lax\n","\n","normalize = lambda x: x / lax.psum(x, axis_name='i')\n","result = pmap(normalize, axis_name='i')(np.arange(4.))\n","print(result)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["[0.         0.16666667 0.33333334 0.5       ]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6jd9DVBQPD-Z","colab_type":"text"},"source":["To use a collective operation like `lax.psum`, you need to supply an `axis_name` argument to `pmap`. The `axis_name` argument associates a name to the mapped axis so that collective operations can refer to it.\n","\n","Another way to write this same code is to use `pmap` as a decorator:"]},{"cell_type":"code","metadata":{"id":"c48qVvlkPF5p","colab_type":"code","outputId":"f76a8ea2-ae7a-4ef2-ce62-7dc48f2ae789","executionInfo":{"status":"ok","timestamp":1573855862041,"user_tz":480,"elapsed":237,"user":{"displayName":"Anselm Levskaya","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB3HU6tluRkaDUKajXgduO8xUvMosX7wCUR9KJm=s64","userId":"09409386770882740563"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from functools import partial\n","\n","@partial(pmap, axis_name='i')\n","def normalize(x):\n","  return x / lax.psum(x, 'i')\n","\n","print(normalize(np.arange(4.)))"],"execution_count":16,"outputs":[{"output_type":"stream","text":["[0.         0.16666667 0.33333334 0.5       ]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3Pr6n8KkOpmz","colab_type":"text"},"source":["Axis names are also important for nested use of `pmap`, where collectives can be applied to distinct mapped axes:"]},{"cell_type":"code","metadata":{"id":"IwoeEd16OrD3","colab_type":"code","outputId":"061840a1-eeb5-46e9-cfea-5ee207f1fe21","executionInfo":{"status":"ok","timestamp":1573855865110,"user_tz":480,"elapsed":433,"user":{"displayName":"Anselm Levskaya","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB3HU6tluRkaDUKajXgduO8xUvMosX7wCUR9KJm=s64","userId":"09409386770882740563"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["@partial(pmap, axis_name='rows')\n","@partial(pmap, axis_name='cols')\n","def f(x):\n","  row_normed = x / lax.psum(x, 'rows')\n","  col_normed = x / lax.psum(x, 'cols')\n","  doubly_normed = x / lax.psum(x, ('rows', 'cols'))\n","  return row_normed, col_normed, doubly_normed\n","\n","x = np.arange(8.).reshape((4, 2))\n","a, b, c = f(x)\n","\n","print(a)\n","print(a.sum(0))"],"execution_count":17,"outputs":[{"output_type":"stream","text":["[[0.         0.0625    ]\n"," [0.16666667 0.1875    ]\n"," [0.33333334 0.3125    ]\n"," [0.5        0.4375    ]]\n","[1. 1.]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Bnc-vlKA6hvI","colab_type":"text"},"source":["When writing nested `pmap` functions in the decorator style, axis names are resolved according to lexical scoping.\n","\n","Check [the JAX reference documentation](https://jax.readthedocs.io/en/latest/jax.lax.html#parallel-operators) for a complete list of the parallel operators. More are being added!\n","\n","Here's how to use `lax.ppermute` to implement a simple halo exchange for a [Rule 30](https://en.wikipedia.org/wiki/Rule_30) simulation:"]},{"cell_type":"code","metadata":{"id":"uazGbMwmf5zO","colab_type":"code","outputId":"71c70679-1f53-4982-d8aa-1555b16a7f84","executionInfo":{"status":"ok","timestamp":1573855915823,"user_tz":480,"elapsed":673,"user":{"displayName":"Anselm Levskaya","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB3HU6tluRkaDUKajXgduO8xUvMosX7wCUR9KJm=s64","userId":"09409386770882740563"}},"colab":{"base_uri":"https://localhost:8080/","height":374}},"source":["from jax.lib import xla_bridge\n","device_count = xla_bridge.device_count()\n","\n","def send_right(x, axis_name):\n","  left_perm = [(i, (i + 1) % device_count) for i in range(device_count)]\n","  return lax.ppermute(x, perm=left_perm, axis_name=axis_name)\n","\n","def send_left(x, axis_name):\n","  left_perm = [((i + 1) % device_count, i) for i in range(device_count)]\n","  return lax.ppermute(x, perm=left_perm, axis_name=axis_name)\n","\n","def update_board(board):\n","  left = board[:-2]\n","  right = board[2:]\n","  center = board[1:-1]\n","  return lax.bitwise_xor(left, lax.bitwise_or(center, right))\n","\n","@partial(pmap, axis_name='i')\n","def step(board_slice):\n","  left, right = board_slice[:1], board_slice[-1:]\n","  right, left = send_left(left, 'i'), send_right(right, 'i')\n","  enlarged_board_slice = np.concatenate([left, board_slice, right])\n","  return update_board(enlarged_board_slice)\n","\n","def print_board(board):\n","  print(''.join('*' if x else ' ' for x in board.ravel()))\n","\n","\n","board = onp.zeros(40, dtype=bool)\n","board[board.shape[0] // 2] = True\n","reshaped_board = board.reshape((device_count, -1))\n","\n","print_board(reshaped_board)\n","for _ in range(20):\n","  reshaped_board = step(reshaped_board)\n","  print_board(reshaped_board)"],"execution_count":18,"outputs":[{"output_type":"stream","text":["                    *                   \n","                   ***                  \n","                  **  *                 \n","                 ** ****                \n","                **  *   *               \n","               ** **** ***              \n","              **  *    *  *             \n","             ** ****  ******            \n","            **  *   ***     *           \n","           ** **** **  *   ***          \n","          **  *    * **** **  *         \n","         ** ****  ** *    * ****        \n","        **  *   ***  **  ** *   *       \n","       ** **** **  *** ***  ** ***      \n","      **  *    * ***   *  ***  *  *     \n","     ** ****  ** *  * *****  *******    \n","    **  *   ***  **** *    ***      *   \n","   ** **** **  ***    **  **  *    ***  \n","  **  *    * ***  *  ** *** ****  **  * \n"," ** ****  ** *  ******  *   *   *** ****\n"," *  *   ***  ****     **** *** **   *   \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KrkEuY3yO7_M","colab_type":"text"},"source":["## Composing with differentiation"]},{"cell_type":"markdown","metadata":{"id":"dGHE7dfypqqU","colab_type":"text"},"source":["As with all things in JAX, you should expect `pmap` to compose with other transformations, including differentiation."]},{"cell_type":"code","metadata":{"id":"VkS7_RcTO_48","colab_type":"code","outputId":"8631b1c5-a4c8-4be0-a0c2-55fa4866d6fd","executionInfo":{"status":"ok","timestamp":1573855919191,"user_tz":480,"elapsed":437,"user":{"displayName":"Anselm Levskaya","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB3HU6tluRkaDUKajXgduO8xUvMosX7wCUR9KJm=s64","userId":"09409386770882740563"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["from jax import grad\n","\n","@pmap\n","def f(x):\n","  y = np.sin(x)\n","  @pmap\n","  def g(z):\n","    return np.cos(z) * np.tan(y.sum()) * np.tanh(x).sum()\n","  return grad(lambda w: np.sum(g(w)))(x)\n","  \n","f(x)"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ShardedDeviceArray([[ 0.        , -0.7170853 ],\n","                    [-3.1085174 , -0.4824318 ],\n","                    [10.366636  , 13.135289  ],\n","                    [ 0.22163185, -0.52112055]], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"4gAJ3QF6PBvi","colab_type":"code","outputId":"37503cb5-29d0-415a-9200-9d4cf6bd79d5","executionInfo":{"status":"ok","timestamp":1573855922730,"user_tz":480,"elapsed":683,"user":{"displayName":"Anselm Levskaya","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB3HU6tluRkaDUKajXgduO8xUvMosX7wCUR9KJm=s64","userId":"09409386770882740563"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["grad(lambda x: np.sum(f(x)))(x)"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ShardedDeviceArray([[ -3.2369726,  -1.6356447],\n","                    [  4.7572474,  11.606951 ],\n","                    [-98.524414 ,  42.76499  ],\n","                    [ -1.6007166,  -1.2568436]], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"8mAz9bEfPl2F","colab_type":"text"},"source":["When reverse-mode differentiating a `pmap` function (e.g. with `grad`), the backward pass of the computation is parallelized just like the forward-pass."]}]}